#include<iostream>
#include<vector>
#include<queue>
#include<stack>
#include<omp.h>

using namespace std;

class Graph{
    public:
    int vertices;
    vector<vector<int>> adj;
    
    Graph(int vertices){
        this->vertices = vertices;
        adj.resize(vertices);
    }
    
    void addEdge(int src, int dest){
        adj[src].push_back(dest);
        adj[dest].push_back(src);
    }
    
    void BFS(int start){
        vector<bool> visited(vertices, false);
        queue<int> q;
        
        visited[start] = true;
        q.push(start);
        
        while(!q.empty()){
            int currVertex = q.front();
            q.pop();
            cout << currVertex << " ";
            
            #pragma omp parallel for
            for(int i = 0; i < adj[currVertex].size(); i++){
                int neighbor = adj[currVertex][i];
                #pragma omp critical
                if(!visited[neighbor]){
                    visited[neighbor] = true;
                    q.push(neighbor);
                }
            }
        }
        cout << endl;
    }
    
    void DFS(int start){
        vector<bool> visited(vertices, false);
        stack<int> s;
        
        visited[start] = true;
        s.push(start);
        
        while(!s.empty()){
            int currVertex = s.top();
            s.pop();
            cout << currVertex << " ";
            
            #pragma omp parallel for
            for(int i = 0; i < adj[currVertex].size(); i++){
                int neighbor = adj[currVertex][i];
                
                if(!visited[neighbor]){
                    visited[neighbor] = true;
                    s.push(neighbor);
                }
            }
        }
        cout << endl ;
    }
};

int main(){
    int n;
    cout << "Enter the no of vertices :- ";
    cin >> n;
    
    Graph graph(n);
    
    int edge;
    cout << "Enter the no of edges :- ";
    cin >> edge;
    
    cout << "Enter the edges (src and dest) " << endl;
    for(int i = 0; i < edge; i++){
        int src, dest;
        cin >> src >> dest;
        graph.addEdge(src, dest);
    }
    
    int start;
    cout << "Enter the starting node :- ";
    cin >> start;
    graph.BFS(start);
    graph.DFS(start);
}



-------------------------------------------------------------------------------------------------------------------



#include<iostream>
#include<ctime>
#include<cstdlib>
#include<omp.h>

using namespace std;

void bubbleSort(int arr[], int size){
    for(int i = 0; i < size; i++){
        for(int j = 0; j < size - i -1; j++){
            if(arr[j] > arr[j+1]){
                int temp = arr[j];
                arr[j] = arr[j+1];
                arr[j+1] = temp;
            }
        }
    }
}

void merge(int arr[], int l, int m, int r){
    int n1 = m -l + 1;
    int n2 = r - m;
    
    int *L = new int[n1];
    int *R = new int[n2];
    
    for(int i = 0; i < n1; i++){
        L[i] = arr[l + i];
    }
    for(int j = 0; j < n2; j++){
        R[j] = arr[m + 1 + j];
    }
    
    int i = 0, j = 0, k = 1;
    
    while(i < n1 && j < n2){
        if(L[i] <= R[j]){
            arr[k++] = L[i++];
        } else {
            arr[k++] = R[j++];
        }
    }
    
    while(i < n1){
        arr[k++] = L[i++];
    }
    while(j < n2){
        arr[k++] = R[j++];
    }
    
    delete []L;
    delete []R;
}

void mergeSort(int arr[],int l, int r){
    if(l < r){
        int m = l + (r -l) / 2;
        
        #pragma omp parallel
        {
            #pragma omp section
            {
                mergeSort(arr, l, m);
            }
            #pragma omp section
            {
                mergeSort(arr, m+1, r);
            }
        }
        
        merge(arr, l, m, r);
    }
}


int main(){
    int n;
    cout << "Enter the size of array :- ";
    cin >> n;
    
    int *arr = new int[n];
    int *arr1 = new int[n];
    
    srand(time(0));
    for(int i = 0; i < n; i++){
        arr[i] = rand() % 100;
        arr1[i] = rand() % 100;
    }
    
    clock_t start = clock();
    bubbleSort(arr, n);
    clock_t end = clock();
    double sbt = double (end - start) / CLOCKS_PER_SEC;
    
    start = clock();
    #pragma omp parallel 
    {
        bubbleSort(arr1, n);
    }
    end = clock();
    double pbt = double (end - start) / CLOCKS_PER_SEC;
    
    start = clock();
    mergeSort(arr, 0, n-1);
    end = clock();
    double smt = double (end - start) / CLOCKS_PER_SEC;
    
    start = clock();
    #pragma omp parallel 
    {
        #pragma omp single
        {
            mergeSort(arr, 0, n-1);       
        }
    }
    end = clock();
    double pmt = double (end - start) / CLOCKS_PER_SEC;
    
    cout << sbt << endl;
    cout << pbt << endl;
    cout << smt << endl;
    cout << pmt << endl;
    
}


--------------------------------------------------------------------------------------------------------------------

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# Load Boston Housing dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data()

print("Training samples:", x_train.shape)
print("Testing samples:", x_test.shape)


# Compute mean and std on training set
mean = x_train.mean(axis=0)
std = x_train.std(axis=0)

# Normalize both train and test data
x_train = (x_train - mean) / std
x_test = (x_test - mean) / std


model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)  # Only one output for price
])


model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)


history = model.fit(
    x_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)


loss, mae = model.evaluate(x_test, y_test, verbose=2)
print(f"Test MAE: {mae:.2f}")


# Plot training and validation loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss During Training')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.legend()
plt.show()

# Plot MAE
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('MAE During Training')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.show()


-----------------------------------------------------------------------------------------------------------------------




import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# Load dataset (only keep top 10,000 most frequent words)
num_words = 10000

(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)

print(f"Training samples: {len(x_train)}")
print(f"Testing samples: {len(x_test)}")


# Set maximum review length
maxlen = 256

# Pad sequences with zeros
x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)


model = keras.Sequential([
    layers.Embedding(num_words, 32, input_length=maxlen),  # Embedding layer
    layers.Flatten(),  # Flatten the 2D embedding to 1D
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Output layer: binary classification
])


model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)


history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=512,
    validation_split=0.2,
    verbose=1
)


loss, accuracy = model.evaluate(x_test, y_test, verbose=2)
print(f"\nTest Accuracy: {accuracy:.2f}")


plt.figure(figsize=(10,5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy During Training')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss During Training')
plt.xlabel('Epochs')
plt.ylabel('Binary Crossentropy Loss')
plt.legend()
plt.grid(True)
plt.show()


--------------------------------------------------------------------------------------------------------


import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np
import os

IMG_SIZE = (128, 128)
BATCH_SIZE = 32

train_dir = os.path.join(extract_path, 'train')   # Assuming folders like /train/<class folders>
test_dir = os.path.join(extract_path, 'test')     # Assuming folders like /test/<class folders>

# Data Augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True,
)

# No augmentation for testing, only rescale
test_datagen = ImageDataGenerator(
    rescale=1./255
)

# Create generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),
    layers.MaxPooling2D(2,2),
    
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    
    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(train_generator.num_classes, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    epochs=15,
    validation_data=test_generator
)

# Plot accuracy
plt.figure(figsize=(10,5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy During Training')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot loss
plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss During Training')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

loss, acc = model.evaluate(test_generator)
print(f"Test Accuracy: {acc:.2f}")

class_names = list(train_generator.class_indices.keys())

# Get a batch of test images
images, labels = next(test_generator)

# Predict first image
pred = model.predict(np.expand_dims(images[0], axis=0))
predicted_class = class_names[np.argmax(pred)]
true_class = class_names[np.argmax(labels[0])]

# Plot
plt.imshow(images[0])
plt.title(f"True: {true_class} | Predicted: {predicted_class}")
plt.axis('off')
plt.show()

-------------------------------------------------------------------------------------------------------------------------------------------


#include <iostream>
#include <cuda.h>
#include <climits>  // Add this

__global__ void minKernel(int* arr, int* result, int n) {  // Fixed __global__
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        atomicMin(result, arr[idx]);
    }
}

__global__ void maxKernel(int* arr, int* result, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        atomicMax(result, arr[idx]);
    }
}

__global__ void sumKernel(int* arr, long long* result, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        atomicAdd(result, arr[idx]);
    }
}

int main() {
    const int arraySize = 9;
    int h_arr[arraySize] = {5, 2, 9, 1, 7, 6, 8, 3, 4};
    int *d_arr, *d_min, *d_max;
    long long *d_sum;

    cudaMalloc(&d_arr, arraySize * sizeof(int));
    cudaMalloc(&d_min, sizeof(int));
    cudaMalloc(&d_max, sizeof(int));
    cudaMalloc(&d_sum, sizeof(long long));

    cudaMemcpy(d_arr, h_arr, arraySize * sizeof(int), cudaMemcpyHostToDevice);

    int initialMin = INT_MAX, initialMax = INT_MIN;
    cudaMemcpy(d_min, &initialMin, sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_max, &initialMax, sizeof(int), cudaMemcpyHostToDevice);
    
    long long initialSum = 0;
    cudaMemcpy(d_sum, &initialSum, sizeof(long long), cudaMemcpyHostToDevice);

    int blockSize = 256;
    int numBlocks = (arraySize + blockSize - 1) / blockSize;

    minKernel<<<numBlocks, blockSize>>>(d_arr, d_min, arraySize);
    maxKernel<<<numBlocks, blockSize>>>(d_arr, d_max, arraySize);
    sumKernel<<<numBlocks, blockSize>>>(d_arr, d_sum, arraySize);

    cudaDeviceSynchronize();  // Optional: good practice to add

    int h_min, h_max;
    long long h_sum;
    cudaMemcpy(&h_min, d_min, sizeof(int), cudaMemcpyDeviceToHost);
    cudaMemcpy(&h_max, d_max, sizeof(int), cudaMemcpyDeviceToHost);
    cudaMemcpy(&h_sum, d_sum, sizeof(long long), cudaMemcpyDeviceToHost);

    double average = static_cast<double>(h_sum) / arraySize;

    std::cout << "Minimum value: " << h_min << std::endl;
    std::cout << "Maximum value: " << h_max << std::endl;
    std::cout << "Sum: " << h_sum << std::endl;
    std::cout << "Average: " << average << std::endl;

    cudaFree(d_arr);
    cudaFree(d_min);
    cudaFree(d_max);
    cudaFree(d_sum);

    return 0;
}

------------------------------------------------------------------------------------------------------------------------------------------

#include <iostream>
#include <cuda.h>
#include <climits> 
#include <chrono>

__global__ void minKernel(int* arr, int* result, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        atomicMin(result, arr[idx]);
    }
}

__global__ void maxKernel(int* arr, int* result, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        atomicMax(result, arr[idx]);
    }
}

__global__ void sumKernel(int* arr, long long* result, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        atomicAdd(result, arr[idx]);
    }
}

int main() {
    const int arraySize = 9;
    int h_arr[arraySize] = {5, 2, 9, 1, 7, 6, 8, 3, 4};

    // Sequential computation
    auto start_cpu = std::chrono::high_resolution_clock::now();

    int minVal = INT_MAX;
    int maxVal = INT_MIN;
    long long sumVal = 0;

    for (int i = 0; i < arraySize; ++i) {
        if (h_arr[i] < minVal) minVal = h_arr[i];
        if (h_arr[i] > maxVal) maxVal = h_arr[i];
        sumVal += h_arr[i];
    }

    double avgVal = static_cast<double>(sumVal) / arraySize;

    auto end_cpu = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double, std::milli> cpu_duration = end_cpu - start_cpu;

    std::cout << "\nSequential Computation Results:" << std::endl;
    std::cout << "Minimum: " << minVal << ", Maximum: " << maxVal << ", Sum: " << sumVal << ", Average: " << avgVal << std::endl;
    std::cout << "Time taken (CPU): " << cpu_duration.count() << " ms" << std::endl;


    // Parallel computation (CUDA)
    int *d_arr, *d_min, *d_max;
    long long *d_sum;
    cudaMalloc(&d_arr, arraySize * sizeof(int));
    cudaMalloc(&d_min, sizeof(int));
    cudaMalloc(&d_max, sizeof(int));
    cudaMalloc(&d_sum, sizeof(long long));

    cudaMemcpy(d_arr, h_arr, arraySize * sizeof(int), cudaMemcpyHostToDevice);

    int initialMin = INT_MAX, initialMax = INT_MIN;
    long long initialSum = 0;
    cudaMemcpy(d_min, &initialMin, sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_max, &initialMax, sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_sum, &initialSum, sizeof(long long), cudaMemcpyHostToDevice);

    int blockSize = 256;
    int numBlocks = (arraySize + blockSize - 1) / blockSize;

    // Create CUDA events for timing
    cudaEvent_t start_gpu, stop_gpu;
    cudaEventCreate(&start_gpu);
    cudaEventCreate(&stop_gpu);

    cudaEventRecord(start_gpu);

    minKernel<<<numBlocks, blockSize>>>(d_arr, d_min, arraySize);
    maxKernel<<<numBlocks, blockSize>>>(d_arr, d_max, arraySize);
    sumKernel<<<numBlocks, blockSize>>>(d_arr, d_sum, arraySize);

    cudaEventRecord(stop_gpu);
    cudaEventSynchronize(stop_gpu);

    float gpuTime = 0;
    cudaEventElapsedTime(&gpuTime, start_gpu, stop_gpu);

    // Copy results back
    int h_min, h_max;
    long long h_sum;
    cudaMemcpy(&h_min, d_min, sizeof(int), cudaMemcpyDeviceToHost);
    cudaMemcpy(&h_max, d_max, sizeof(int), cudaMemcpyDeviceToHost);
    cudaMemcpy(&h_sum, d_sum, sizeof(long long), cudaMemcpyDeviceToHost);

    double h_avg = static_cast<double>(h_sum) / arraySize;

    std::cout << "\nParallel Computation (CUDA) Results:" << std::endl;
    std::cout << "Minimum: " << h_min << ", Maximum: " << h_max << ", Sum: " << h_sum << ", Average: " << h_avg << std::endl;
    std::cout << "Time taken (GPU): " << gpuTime << " ms" << std::endl;

    // Cleanup
    cudaFree(d_arr);
    cudaFree(d_min);
    cudaFree(d_max);
    cudaFree(d_sum);
    cudaEventDestroy(start_gpu);
    cudaEventDestroy(stop_gpu);

    return 0;
}

--------_----=---==--------



# Predict probabilities
y_pred_probs = model.predict(x_test)

# Convert probabilities to 0/1
y_pred = (y_pred_probs > 0.5).astype("int32").flatten()

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix (Actual numbers):")
print(cm)

# Extract values
TN, FP, FN, TP = cm.ravel()
print(f"\nTrue Negatives: {TN}")
print(f"False Positives: {FP}")
print(f"False Negatives: {FN}")
print(f"True Positives: {TP}")

# Calculate metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)

print(f"\nPrecision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Accuracy: {acc:.4f}")
